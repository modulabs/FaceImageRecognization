{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages loaded\n",
      "['trainlabel', 'trainimg', 'imgsize', 'testimg', 'testlabel']\n",
      "2614 train images loaded\n",
      "1743 test images loaded\n",
      "2304 dimensional input\n",
      "Image size is [48 48]\n",
      "8 classes\n",
      "NETWORK READY\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "# modulabs setting\n",
    "sys.path.insert(0, \"/home/modu/DemoDay_ImageProcessing_Team/gwansang/GenderRecognition\")\n",
    "sys.path.insert(0, \"/home/modu/DemoDay_ImageProcessing_Team/gwansang/AgeEstimation/\")\n",
    "sys.path.insert(0, \"/home/modu/DemoDay_ImageProcessing_Team/gwansang/EmotionEstimation/\")\n",
    "\n",
    "# my computer setting\n",
    "#sys.path.insert(0, \"/home/chanwoo/Sources/gwansang/GenderRecognition/\")\n",
    "#sys.path.insert(0, \"/home/chanwoo/Sources/gwansang/AgeEstimation/\")\n",
    "#sys.path.insert(0, \"/home/chanwoo/Sources/gwansang/EmotionEstimation/\")\n",
    "\n",
    "import VGG_RGB\n",
    "import AgeEstimation_prediction\n",
    "import emotion_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchSize = 1\n",
    "GenderImageSize = 224\n",
    "AgeImageSize = 48\n",
    "EmotionImageSize = 48\n",
    "\n",
    "epochNum = 0\n",
    "num_class = 2\n",
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "directoryName = \"face data/large\"\n",
    "\n",
    "# check points\n",
    "GenderCheckPoint_dir = \"GenderRecognition/\"\n",
    "AgeCheckPoint_dir = \"AgeEstimation/\"\n",
    "EmotionCheckPoint_dir = \"EmotionEstimation/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender Recognition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "npy file loaded\n",
      "construction FCN32VGG_RGB\n",
      "Layer name: conv1_1\n",
      "Layer shape: (3, 3, 3, 64)\n",
      "Layer name: conv1_2\n",
      "Layer shape: (3, 3, 64, 64)\n",
      "Layer name: conv2_1\n",
      "Layer shape: (3, 3, 64, 128)\n",
      "Layer name: conv2_2\n",
      "Layer shape: (3, 3, 128, 128)\n",
      "Layer name: conv3_1\n",
      "Layer shape: (3, 3, 128, 256)\n",
      "Layer name: conv3_2\n",
      "Layer shape: (3, 3, 256, 256)\n",
      "Layer name: conv3_3\n",
      "Layer shape: (3, 3, 256, 256)\n",
      "Layer name: conv4_1\n",
      "Layer shape: (3, 3, 256, 512)\n",
      "Layer name: conv4_2\n",
      "Layer shape: (3, 3, 512, 512)\n",
      "Layer name: conv4_3\n",
      "Layer shape: (3, 3, 512, 512)\n",
      "Layer name: conv5_1\n",
      "Layer shape: (3, 3, 512, 512)\n",
      "Layer name: conv5_2\n",
      "Layer shape: (3, 3, 512, 512)\n",
      "Layer name: conv5_3\n",
      "Layer shape: (3, 3, 512, 512)\n",
      "Layer name: fc6\n",
      "Layer shape: [25088, 4096]\n",
      "Layer name: fc7\n",
      "Layer name: fc8\n",
      "Layer shape: [4096, 1000]\n",
      "shape of logits: (1, 2)\n",
      "shape of labels: (1,)\n",
      "load Gender model\n"
     ]
    }
   ],
   "source": [
    "# Create Image space\n",
    "GenderValidateImages = np.zeros([1, GenderImageSize, GenderImageSize, 3], dtype=np.float32)\n",
    "\n",
    "# Create Gender Class\n",
    "vgg_rgb = VGG_RGB.VGG_RGB()\n",
    "\n",
    "\n",
    "# GenderRecognision input arguments\n",
    "GenderImage = tf.placeholder(tf.float32, [batchSize, GenderImageSize, GenderImageSize, 3])\n",
    "GenderLabel = tf.placeholder(tf.int32, [batchSize, 1])\n",
    "Gender_is_train = tf.placeholder(tf.bool)\n",
    "\n",
    "\n",
    "# building each model \n",
    "GenderRecognitionSession = tf.Session()\n",
    "vgg_rgb.build(GenderImage, GenderLabel, Gender_is_train, scope=\"VGG_RGB\",  num_classes=num_class, debug=True)\n",
    "\n",
    "# load model\n",
    "GenderRecognitionSaver = tf.train.Saver()\n",
    "Genderckpt = tf.train.get_checkpoint_state(GenderCheckPoint_dir)\n",
    "\n",
    "if Genderckpt and Genderckpt.model_checkpoint_path:\n",
    "    GenderRecognitionSaver.restore(GenderRecognitionSession, Genderckpt.model_checkpoint_path)\n",
    "    print ('load Gender model') \n",
    "    \n",
    "else:\n",
    "    print ('load fail')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotion Estimation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load emotion model\n"
     ]
    }
   ],
   "source": [
    "# Create image space\n",
    "EmotionValidateImages = np.zeros([1, EmotionImageSize, EmotionImageSize], dtype=np.float32)\n",
    "\n",
    "# Create Emotion Class \n",
    "#EmotionEstimation = Emotion_Estimation()\n",
    "\n",
    "#EmotionEstimation input arguments\n",
    "EmotionInputImage = tf.placeholder( tf.float32, [batchSize, EmotionImageSize, EmotionImageSize ] )\n",
    "keepratio = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "# building model \n",
    "EmotionEstimationSession = tf.Session()\n",
    "EmotionPred = emotion_cnn.conv_basic( EmotionInputImage )\n",
    "\n",
    "# load model\n",
    "EmotionEstimationSaver = tf.train.Saver(emotion_cnn.weight_list)\n",
    "\n",
    "Emotionckpt = tf.train.get_checkpoint_state(EmotionCheckPoint_dir)\n",
    "\n",
    "if Emotionckpt and Emotionckpt.model_checkpoint_path:\n",
    "    EmotionEstimationSaver.restore(EmotionEstimationSession, Emotionckpt.model_checkpoint_path)\n",
    "    print ('load emotion model' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age Estimation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tensorflow.python.ops.variables.Variable object at 0x7f8b081daa90>, <tensorflow.python.ops.variables.Variable object at 0x7f8b081de2d0>, <tensorflow.python.ops.variables.Variable object at 0x7f8b0817aed0>, <tensorflow.python.ops.variables.Variable object at 0x7f8b0819d6d0>, <tensorflow.python.ops.variables.Variable object at 0x7f8b0818d9d0>, <tensorflow.python.ops.variables.Variable object at 0x7f8b081dae90>, <tensorflow.python.ops.variables.Variable object at 0x7f8af006bc50>, <tensorflow.python.ops.variables.Variable object at 0x7f8af005bfd0>, <tensorflow.python.ops.variables.Variable object at 0x7f8af005b290>, <tensorflow.python.ops.variables.Variable object at 0x7f8af005bf90>, <tensorflow.python.ops.variables.Variable object at 0x7f8af000b250>, <tensorflow.python.ops.variables.Variable object at 0x7f8ae82ff6d0>, <tensorflow.python.ops.variables.Variable object at 0x7f8af003c490>]\n",
      "load Age model\n"
     ]
    }
   ],
   "source": [
    "# create image space \n",
    "AgeValidateImages = np.zeros([1, AgeImageSize, AgeImageSize ], dtype=np.float32)\n",
    "\n",
    "# Create Age Class\n",
    "#AgeEstimation = AgeEstimation_prediction.AgeEstimation()\n",
    "\n",
    "# AgeEstimation input arguments\n",
    "#X=tf.placeholder(dtype=tf.float32, shape=[None,48,48,1])\n",
    "AgeInputImage = tf.placeholder(tf.float32, [batchSize, AgeImageSize, AgeImageSize, 1] )\n",
    "\n",
    "\n",
    "# building model\n",
    "AgeEstimationSession = tf.Session()\n",
    "AgePred = AgeEstimation_prediction.model( AgeInputImage )\n",
    "\n",
    "# load model\n",
    "print AgeEstimation_prediction.weight_list\n",
    "AgeEstimationSaver = tf.train.Saver(AgeEstimation_prediction.weight_list)\n",
    "\n",
    "\n",
    "Ageckpt = tf.train.get_checkpoint_state(AgeCheckPoint_dir)\n",
    "\n",
    "if Ageckpt and Ageckpt.model_checkpoint_path:\n",
    "    AgeEstimationSaver.restore(AgeEstimationSession, Ageckpt.model_checkpoint_path)\n",
    "    print ('load Age model' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-7-0a0b4cd960f6>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-0a0b4cd960f6>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    ret, frame = cap.read()\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "print \"camera start ==========================\"\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "newXSize = 100\n",
    "newYSize = 100\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame,1)\n",
    "    height, width, channels = frame.shape\n",
    "    recframe = frame.copy()\n",
    "    roi_color = np.zeros((1, 1, 3), np.uint8)\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    key = cv2.waitKey(1)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "    if(w<width/5):\n",
    "            break\n",
    "        cv2.rectangle(recframe, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        crop_img = frame[y:y + h, x:x + w]\n",
    "\n",
    "        # Capture\n",
    "        if key == ord('c'):\n",
    "            cv2.imshow(\"roi\", crop_img)\n",
    "            resize_Image = cv2.resize(crop_img,(newXSize, newYSize))\n",
    "\n",
    "            ##############################\n",
    "            ##############################\n",
    "            ##############################\n",
    "            # add deep learning function #\n",
    "            #    using \"resize_Image\"    #\n",
    "            ##############################\n",
    "            ##############################\n",
    "            ##############################\n",
    "            \n",
    "            # resizing images\n",
    "            Gender_resized_img = cv2.resize(crop_img, (GenderImageSize, GenderImageSize, 3)) \n",
    "            Age_resized_img = cv2.resize(crop_img, (AgeImageSize, AgeImageSize ))\n",
    "            Emotion_resized_img = cv2.resize(crop_img, (EmotionImagesize, EmotionImageSize))\n",
    "\n",
    "            #rgb2gray emotion and age images\n",
    "            #.....\n",
    "            #\n",
    "\n",
    "            GenderValidateImages[0, :, :, :] = Gender_resized_img\n",
    "            AgeValidateImages[0, :, :] = Age_resized_img\n",
    "            EmotionValidateImages[0, :, :] = Emotion_resized_img\n",
    "\n",
    "                        # Predict gender\n",
    "            result = GenderRecognitionSession.run(vgg_rgb.pred, feed_dict={X: ValidateImages, Gender_is_train: False})\n",
    "            AgeResult = AgeEstimationSession.run( AgePred, feed_dict={AgeInputImage: AgeValidateImages } )\n",
    "            EmotionResult = EmotionEstimationSession.run( EmotionPred, feed_dict={EmotionInputImage: EmotionValidateImages } )\n",
    "            \n",
    "            print \"gen : \", result\n",
    "            print \"age : \", AgeResult\n",
    "            print \"emo : \", EmotionResult\n",
    "            \n",
    "            predict = result[0]\n",
    "\n",
    "            # print strings\n",
    "            print (\"predict : \" ) + str(predict)\n",
    "            if predict==0:\n",
    "                text = \"Male\"\n",
    "            else:\n",
    "                text = \"Female\"\n",
    "\n",
    "            # Draw rectangle and put text on frame\n",
    "            cv2.rectangle(frame, (x,y), (x2,y2), (0,255,0),3)\n",
    "            cv2.putText(frame,text,(x2+10,y2-10), font, 1,(0,0,0),3)\n",
    "            break\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame',recframe)\n",
    "\n",
    "    # EXIT\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "GenderRecognitionSession.close()\n",
    "AgeEstimationSession.close()\n",
    "EmotionEstimationSession.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
